<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-05-12T13:35:11.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>John Doe</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>分布式锁</title>
    <link href="http://yoursite.com/2020/05/12/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/"/>
    <id>http://yoursite.com/2020/05/12/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/</id>
    <published>2020-05-12T13:35:11.000Z</published>
    <updated>2020-05-12T13:35:11.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="redis"><a href="#redis" class="headerlink" title="redis"></a>redis</h3><ol><li>set key val nx ex</li></ol><ul><li>优点：实现简单，性能好</li><li>缺点：超时时间不好控制，极端情况，会出现超时后，多个节点获取到同一把锁的情况。</li></ul><ol start="2"><li>问题<ol><li>主从，redis主从采用异步复制，那么如果主机宕机，切换到从，会导致部分锁数据丢失。此时，多个client会拿到同一把锁。</li><li>如果锁没有设置超时，若client挂掉，则锁永远不会释放</li><li>如果锁设置了超时，若client阻塞或业务执行超时，也会导致多个client拿到同一把锁。</li></ol></li></ol><h3 id="zookeeper"><a href="#zookeeper" class="headerlink" title="zookeeper"></a>zookeeper</h3><ol><li>使用临时顺序节点，如果自己是子节点的第一个，则表示加锁成功。否则，watch上一个，如果上一个释放，表示轮到自己了。</li></ol><ul><li>优点：一般情况，不存在client宕机/超时问题，zk感知到client宕机，会自动删除对应的临时顺序节点，相当于自动释放锁，或者取消自己的排队。</li><li>缺点：实现复杂，吞吐量不高</li></ul><ol start="2"><li>问题<ol><li>因为zk使用心跳判断client是否在线，如果网络超时或者full GC等等，导致zk认为client宕机，则会释放锁。导致其他client同时获得该锁。<strong>但是这种情况很少见，相比之下，client处理超时这种更常见，这也是zk比redis方案好的原因。</strong></li></ol></li></ol><h3 id="mysql行锁"><a href="#mysql行锁" class="headerlink" title="mysql行锁"></a>mysql行锁</h3><ul><li>优点：不需引入额外中间件</li><li>缺点：吞吐量不高；也存在client宕机超时问题</li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ol><li>探测client是否宕机很难，如果因为超时，那就不应该释放锁。如果是因为宕机，那就应该释放锁。</li><li><code>没有完美的方案，实际场景中，分布式锁只应作为辅助手段，比如为了减少DB的压力等，不应仅靠它控制业务并发逻辑。</code></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;redis&quot;&gt;&lt;a href=&quot;#redis&quot; class=&quot;headerlink&quot; title=&quot;redis&quot;&gt;&lt;/a&gt;redis&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;set key val nx ex&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;优点：实现简单，性能好&lt;/l
      
    
    </summary>
    
    
      <category term="分布式" scheme="http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
      <category term="方案总结" scheme="http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/%E6%96%B9%E6%A1%88%E6%80%BB%E7%BB%93/"/>
    
    
  </entry>
  
  <entry>
    <title>高并发常见方案</title>
    <link href="http://yoursite.com/2020/05/12/%E9%AB%98%E5%B9%B6%E5%8F%91/"/>
    <id>http://yoursite.com/2020/05/12/%E9%AB%98%E5%B9%B6%E5%8F%91/</id>
    <published>2020-05-12T13:29:38.000Z</published>
    <updated>2020-05-12T13:29:38.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="高并发写"><a href="#高并发写" class="headerlink" title="高并发写"></a>高并发写</h3><ol><li>数据分片<ul><li>数据库分库分表</li><li>JDK concurrentHashMap实现</li><li>kafka的partition</li><li>ES的分布式索引</li></ul></li><li>任务分片<ul><li>CPU的指令流水线</li><li>Map/Reduce</li><li>Tomcat 1+N+M 网络模型：1个监听线程，N个IO线程负责对socket进行读写，M个worker对请求做逻辑处理。</li></ul></li><li>异步化：异步接口、异步IO<ul><li>短信验证码注册/登录</li><li>订单系统</li><li>广告计费系统，异步，多消息合并扣费</li><li>Kafka的Pipeline</li></ul></li><li>WAL技术<ul><li>数据库redo log</li><li>LSM树 </li></ul></li><li>批量<ul><li>kafka的百万qps写入:partition分片，磁盘顺序写入，批量（leader/follower之间的批量，本地client之间的批量）</li><li>mysql的group commit机制，对多事务的redo log批量flush</li></ul></li></ol><h3 id="高并发读"><a href="#高并发读" class="headerlink" title="高并发读"></a>高并发读</h3><ol><li>加缓存<ul><li>本地缓存/redis/memcached</li></ul></li><li>增加副本冗余<ul><li>MySQL master/slave</li><li>CDN 静态文件加速</li></ul></li><li>并发读<ul><li>异步RPC</li><li>冗余请求，降低失败率</li></ul></li></ol><h3 id=""><a href="#" class="headerlink" title=""></a></h3>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;高并发写&quot;&gt;&lt;a href=&quot;#高并发写&quot; class=&quot;headerlink&quot; title=&quot;高并发写&quot;&gt;&lt;/a&gt;高并发写&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;数据分片&lt;ul&gt;
&lt;li&gt;数据库分库分表&lt;/li&gt;
&lt;li&gt;JDK concurrentHashMap实现&lt;/l
      
    
    </summary>
    
    
      <category term="分布式" scheme="http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
      <category term="方案总结" scheme="http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/%E6%96%B9%E6%A1%88%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="分布式" scheme="http://yoursite.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>多副本一致性</title>
    <link href="http://yoursite.com/2020/05/12/%E5%A4%9A%E5%89%AF%E6%9C%AC%E4%B8%80%E8%87%B4%E6%80%A7/"/>
    <id>http://yoursite.com/2020/05/12/%E5%A4%9A%E5%89%AF%E6%9C%AC%E4%B8%80%E8%87%B4%E6%80%A7/</id>
    <published>2020-05-12T03:23:33.000Z</published>
    <updated>2020-05-12T05:09:02.394Z</updated>
    
    <content type="html"><![CDATA[<h3 id="同步本质"><a href="#同步本质" class="headerlink" title="同步本质"></a>同步本质</h3><p>每台机器都把收到的请求按日志存下来，各机器的日志文件保持一致。选择存储“事件流”，而非最终状态，原因是：</p><ol><li>日志只有一种操作，append，相对简单</li></ol><h3 id="Paxos算法"><a href="#Paxos算法" class="headerlink" title="Paxos算法"></a>Paxos算法</h3><h4 id="1-Basic-Paxos"><a href="#1-Basic-Paxos" class="headerlink" title="1. Basic Paxos"></a>1. Basic Paxos</h4><ol><li>两个角色，Proposer 和 Acceptor，以及一个自增ID（n）</li><li>两个阶段，Propose阶段 和 Accept 阶段</li><li>Propose阶段<ol><li>proposer广播消息，id为n，prepare(n)</li><li>acceptor接收消息，如果n &gt; local N，则回复yes</li><li>proposer收到半数以上的yes，开始广播，否则id自增，重新广播</li></ol></li><li>Acctpt阶段<ol><li>proposer广播消息, accept(n, value)</li><li>acceptor接收消息，如果n &gt; loacal N，则持久化，返回yes</li><li>proposer收到半数以上的yes，则结束。否则id自增，从proposer阶段重新开始。</li></ol></li><li>两个问题<ol><li>Paxos是个不断循环的2PC，有可能陷入死循环，所谓“活锁”。比如3个node同时propose，都收到no，又同时n++，继续propose，继续no</li><li>性能：每次写入，需要两次RTT + 两次写盘。两次RTT分别是Propose/Accept阶段。这两个阶段都会持久化一些变量，需要磁盘IO。</li></ol></li><li>活锁问题<ol><li>多点写入，变为单点写入。选出一个leader，只让leader当proposer。从而减少冲突。leader选取办法，比如每个节点增加编号，使用心跳，选取编号最大的节点为leader。即使出现同一时间，多个leader，也不影响paxos的正确性，只会增大并发写冲突的概率。</li></ol></li></ol><h3 id="Raft算法"><a href="#Raft算法" class="headerlink" title="Raft算法"></a>Raft算法</h3><ol><li>单点写入：任一时刻，只允许一个有效的leader存在，所有的写请求，都传到leader上，然后由leader同步给超过半数的follower。</li><li>单条日志结构：term + index + content。term是leader的任期，只会单调递增；index是日志顺序编号，也是递增；</li><li>分为三个阶段，选举阶段，正常阶段，恢复阶段</li><li>选举阶段<ol><li>节点有三个状态：leader、follower、candidate。candidate是个中间状态。</li><li>当follower在一定时间收不到leader心跳时，就会随机sleep一个时间，然后变为candidate，发起选举。选举结束后，变为leader或follower。</li><li>选举算法，保证同一时间只有一个leader。<ol><li>如果选举请求里，日志的term和index比自己本地的新，则返回true，否则返回false。</li><li>candidate收到多数派返回true，则成为leader</li><li>每个节点只能投一次true，防止多个leader。因此选取出的leader不一定是最新的，但一定比大多数节点新。</li></ol></li></ol></li><li>正常阶段，复制日志<ol><li>只要超过半数的follower复制成功，就返回给客户端日志写入成功。</li><li>关键的日志一致性保证：<blockquote><ol><li>如果两个节点的日志，index和term相同，则内容一定相同。</li><li>如果index=M处的日志相同，则在M之前的日志，也一定相同。</li></ol></blockquote></li></ol></li><li>恢复阶段<ol><li>leader同步term给follower</li><li>以leader本地的日志为基准，复制给follower</li></ol></li><li>安全性保证<ol><li>leader数据是基准，leader不会从别的节点同步数据，只会是别的节点根据leader数据删除或追加自己的数据。</li><li>对于已经commit的日志，一定是commit的。对于新任leader上，前任leader未commit的日志，稍后会变为commit状态。不在新任leader上的未commit数据，会被覆盖。</li></ol></li></ol><h3 id="Zab"><a href="#Zab" class="headerlink" title="Zab"></a>Zab</h3><p>zookeeper使用的强一致性算法，同时也是单点写入，写请求都转发给leader。</p><ol><li>模型对比，复制状态机(replicated state machine, paxos/raft) vs 主备系统（primay-backup system，zab）,前者持久化的是客户端的请求序列（日志序列），另外一个持久化的是数据的状态变化。<ol><li>数据同步次数不一样，如果client执行三次x=1，后两次在主备系统里，不用触发同步。</li><li>存储状态变化，具有幂等性，而复制状态机不具备。</li></ol></li><li>zxid<ol><li>高32位，leader任期，类似raft的term</li><li>低32位，日志序列，类似raft的日志index</li></ol></li><li>三个阶段：Leader选举，BroadCast,恢复阶段</li><li>Leader选举：FLE算法<ol><li>Leader和Follower之间是双向心跳；raft里是单向</li><li>选取zxid最大的节点作为leader；和raft选取term+index最新的节点作为leader一个意思。</li></ol></li><li>broadcast阶段<ol><li></li></ol></li></ol><h3 id="raft-vs-zab"><a href="#raft-vs-zab" class="headerlink" title="raft vs zab"></a>raft vs zab</h3><p>参考：<a href="https://my.oschina.net/pingpangkuangmo/blog/782702" target="_blank" rel="noopener">https://my.oschina.net/pingpangkuangmo/blog/782702</a></p><ol><li>上一轮残留的数据怎么处理？</li></ol><p>首先看下上一轮次的leader在挂或者失去leader位置之前，会有哪些数据？</p><ul><li>已过半复制的日志</li><li>未过半复制的日志<br>一个日志是否被过半复制，是否被提交，这些信息是由leader才能知晓的，</li></ul><p>那么下一个leader该如何来判定这些日志呢？</p><p>下面分别来看看Raft和ZooKeeper的处理策略：</p><p>Raft：对于之前term的过半或未过半复制的日志采取的是保守的策略，全部判定为未提交，只有当当前term的日志过半了，才会顺便将之前term的日志进行提交</p><p>ZooKeeper：采取激进的策略，对于所有过半还是未过半的日志都判定为提交，都将其应用到状态机中</p><p>Raft的保守策略更多是因为Raft在leader选举完成之后，没有同步更新过程来保持和leader一致（在可以对外服务之前的这一同步过程）。而ZooKeeper是有该过程的</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;同步本质&quot;&gt;&lt;a href=&quot;#同步本质&quot; class=&quot;headerlink&quot; title=&quot;同步本质&quot;&gt;&lt;/a&gt;同步本质&lt;/h3&gt;&lt;p&gt;每台机器都把收到的请求按日志存下来，各机器的日志文件保持一致。选择存储“事件流”，而非最终状态，原因是：&lt;/p&gt;
&lt;ol&gt;

      
    
    </summary>
    
    
      <category term="后端" scheme="http://yoursite.com/categories/%E5%90%8E%E7%AB%AF/"/>
    
      <category term="系统原理" scheme="http://yoursite.com/categories/%E5%90%8E%E7%AB%AF/%E7%B3%BB%E7%BB%9F%E5%8E%9F%E7%90%86/"/>
    
    
      <category term="分布式" scheme="http://yoursite.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>开天辟地.md</title>
    <link href="http://yoursite.com/2020/05/11/%E5%BC%80%E5%A4%A9%E8%BE%9F%E5%9C%B0/"/>
    <id>http://yoursite.com/2020/05/11/%E5%BC%80%E5%A4%A9%E8%BE%9F%E5%9C%B0/</id>
    <published>2020-05-11T09:05:19.000Z</published>
    <updated>2020-05-11T09:13:22.465Z</updated>
    
    <content type="html"><![CDATA[<p>欢迎</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;欢迎&lt;/p&gt;

      
    
    </summary>
    
    
    
      <category term="tag1" scheme="http://yoursite.com/tags/tag1/"/>
    
  </entry>
  
</feed>
